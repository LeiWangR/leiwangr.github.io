---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

<style>
a:link {
  text-decoration: none;
}

a:visited {
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

a:active {
  text-decoration: underline;
}
</style>

<!-- {% if author.googlescholar %}
  You can also find my articles on [my Google Scholar profile](https://scholar.google.com/citations?user=VWCZLXgAAAAJ&hl=en).
{% endif %}

{% include base_path %}

{% for post in site.publications reversed %} 
  {% include archive-single.html %}
{% endfor %} -->

<!-- You can find my published articles on my [Google Scholar](https://scholar.google.com/citations?user=VWCZLXgAAAAJ&hl=en) profile. -->

$^\dagger$: Corresponding author.

<h2>Journals</h2>

Journal impact factors follow the [Clarivate JCR 2023](https://jcr.clarivate.com) system.

<table id="gsc_a_t">
	<tbody id="gsc_a_b">
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://arxiv.org/abs/2505.16702"><strong><span class="gsc_a_at">Truth and Trust: Fake News Detection via Biosignals</span></strong></a>
				<div class="gs_gray">G Nguyen, <strong>L Wang</strong>, Y Jiang, T Gedeon</div>
				<div class="gs_gray">arXiv preprint arXiv:2505.16702</div>
			</td>
			<td class="gsc_a_c">Under major revision</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr>
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://link.springer.com/article/10.1007/s11263-025-02513-4"><strong><span class="gsc_a_at">Feature Hallucination for Self-supervised Action Recognition</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>$^\dagger$, P Koniusz</div>
				<div class="gs_gray">International Journal of Computer Vision (<strong>IJCV</strong>), 1-34</div>
			</td>
			<td class="gsc_a_c">(Extension of our ICCV'19 and ACMMM'21)<br>[<font color="red"><strong>IF: 11.6</strong></font>]</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E0FFFF"> 
			<td class="gsc_a_t"><a href="https://link.springer.com/article/10.1007/s11263-024-02070-2"><strong><span class="gsc_a_at">Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal‑Viewpoint Alignment</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>$^\dagger$, J Liu, L Zheng, T Gedeon, P Koniusz</div>
				<div class="gs_gray">International Journal of Computer Vision (<strong>IJCV</strong>), 4091–4122</div>
			</td>
			<td class="gsc_a_c">(IJCV special issue on our ACCV'22)<br>[<font color="red"><strong>IF: 11.6</strong></font>] <br><a href="https://github.com/LeiWangR/JEANIE" style="color:#000000;"> Code </a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2024</span></td>
		</tr>
                <tr class="gsc_a_tr" style="background-color:#E5E4E2">
			<td class="gsc_a_t"><a href="https://ieeexplore.ieee.org/document/9895208"><strong><span class="gsc_a_at">Fusing Higher-Order Features in Graph Neural Networks for Skeleton-Based Action Recognition</span></strong></a>
				<div class="gs_gray">Z Qin, Y Liu, P Ji, D Kim, <strong>L Wang</strong>, B McKay, S Anwar, T Gedeon</div>
				<div class="gs_gray">IEEE Transactions on Neural Networks and Learning Systems (<strong>TNNLS</strong>), 4783-4797</div>
			</td>
			<td class="gsc_a_c">[<strong><font color="red">IF: 10.2</font></strong>]<br><a href="https://github.com/harutatsuakiyama/Angular-Skeleton-Encoding" style="color:#000000;">Code</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2021</span></td>
		</tr>
                <tr class="gsc_a_tr" style="background-color:#E0FFFF">
			<td class="gsc_a_t"><strong data-bind="text: title"><a href="https://ieeexplore.ieee.org/document/9521829">Tensor Representations for Action Recognition</a></strong>
				<div class="gs_gray">P Koniusz,<strong> L Wang</strong>, A Cherian</div>
				<div class="gs_gray">IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>) 44 (2), 648-665</div>
			</td>
			<td class="gsc_a_c">[<strong><font color="red">IF: 20.8</font></strong>]</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2020</span></td>
		</tr>
                <tr class="gsc_a_tr" style="background-color:#E0FFFF">
			<td class="gsc_a_t"><a href="https://ieeexplore.ieee.org/document/8753686"><strong><span class="gsc_a_at">A Comparative Review of Recent Kinect-based Action Recognition Algorithms</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>$^\dagger$, DQ Huynh, P Koniusz</div>
				<div class="gs_gray">IEEE Transactions on Image Processing (<strong>TIP</strong>) 29 (1), 15-28</div>
			</td>
			<td class="gsc_a_c">[<strong><font color="red">IF: 10.8</font></strong>]<br><a href="https://github.com/LeiWangR/HDG" style="color:#000000;">Dataset & Code</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2020</span></td>
		</tr>
        </tbody>
</table>  

<h2>Conferences</h2>

Conference rankings follow the [CORE 2023](https://portal.core.edu.au/conf-ranks/) system.

<table id="gsc_a_t">
	<tbody id="gsc_a_b">
		<tr class="gsc_a_tr" style="background-color:#E5E4E2">
			<td class="gsc_a_t"><a href=""><strong><span class="gsc_a_at">Physics-Informed Representation Alignment for Sparse Radio-Map Reconstruction</span></strong></a>
				<div class="gs_gray">J Haozhe*, W Chen*, Z Huang*, <strong>L Wang</strong>, H Xiao, N Jia, K Wu, S Lai, B Tian, Y Yue</div>
				<div class="gs_gray">ACM Multimedia (<strong>ACM-MM</strong>)</div>
			</td>
			<td class="gsc_a_c">(* denotes equal contribution.)<br>Brave New Ideas Track<br>[<font color="red"><strong>A*</strong>, oral</font>]<br><a href="https://github.com/Hxxxz0/RMDM" style="color:#000000;">Code</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr>
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://arxiv.org/abs/2505.12552"><strong><span class="gsc_a_at">FreqSelect: Frequency-Aware fMRI-to-Image Reconstruction</span></strong></a>
				<div class="gs_gray">J Ye, <strong>L Wang</strong>$^\dagger$, MZ Hossain</div>
				<div class="gs_gray">British Machine Vision Conference (<strong>BMVC</strong>)</div>
			</td>
			<td class="gsc_a_c">[<font color="red"><strong>A</strong></font>]<br><a href="https://leiwangr.github.io/files/bmvc25.pdf" style="color:#000000;">Main paper + Appendix</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E5E4E2">
			<td class="gsc_a_t"><a href="https://www.arxiv.org/abs/2506.02452"><strong><span class="gsc_a_at">ANT: Adaptive Neural Temporal-Aware Text-to-Motion Model</span></strong></a>
				<div class="gs_gray">W Chen*, K Yu*, J Haozhe*, K Yuan, Z Huang, B Tian, S Lai, H Xiao, E Zhang, <strong>L Wang</strong>, Y Yue</div>
				<div class="gs_gray">ACM Multimedia (<strong>ACM-MM</strong>)</div>
			</td>
			<td class="gsc_a_c">(* denotes equal contribution.)<br>[<font color="red"><strong>A*</strong></font>]<br><a href="https://github.com/CCSCovenant/ANT" style="color:#000000;">Code</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr>
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2410.01506"><strong><span class="gsc_a_at">Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion</span></strong></a>
				<div class="gs_gray">D Ding, <strong>L Wang</strong>$^\dagger$, L Zhu, T Gedeon, P Koniusz</div>
				<div class="gs_gray">International Conference on Learning Representations (<strong>ICLR</strong>)</div>
			</td>
			<td class="gsc_a_c">[<font color="red"><strong>A*</strong></font>]</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#FFFFE0"> 
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2409.14543"><strong><span class="gsc_a_at">TrackNetV4: Enhancing Fast Sports Object Tracking with Motion Attention Maps</span></strong></a>
				<div class="gs_gray">A Raj, <strong>L Wang</strong>$^\dagger$, T Gedeon</div>
				<div class="gs_gray">IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 1-5</div>
			</td>
			<td class="gsc_a_c">[<font color="red"><strong>B</strong></font>]<br><a href="https://github.com/TrackNetV4/TrackNetV4" style="color:#000000;">Code</a>, <a href="https://tracknetv4.github.io/" style="color:#000000;">Project website</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#FFFFE0"> 
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2402.04857"><strong><span class="gsc_a_at">Advancing Video Anomaly Detection: A Concise Review and a New Dataset</span></strong></a>
				<div class="gs_gray">L Zhu, <strong>L Wang</strong>$^\dagger$, A Raj, T Gedeon, C Chen</div>
				<div class="gs_gray">Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), 89943-89977</div>
			</td>
			<td class="gsc_a_c">[<font color="red"><strong>A*</strong></font>]<br><a href="https://github.com/Tom-roujiang/MSAD" style="color:#000000;">Code</a>, <a href="https://time.griffith.edu.au/paper-sites/msad/" style="color:#000000;">Project website</a>, <a href="https://leiwangr.github.io/files/97585.png" style="color:#000000;">Poster</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2024</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#FFFFE0">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2407.03179"><strong><span class="gsc_a_at">Motion meets Attention: Video Motion Prompts</span></strong></a>
				<div class="gs_gray">Q Chen, <strong>L Wang</strong>$^\dagger$, P Koniusz, T Gedeon</div>
				<div class="gs_gray">Asian Conference on Machine Learning (<strong>ACML</strong>), 591-606</div>
			</td>
			<td class="gsc_a_c">[<font color="red">Long presentation, <br>Conf. track: 20.11% accept.,<br>6.52% long pres.</font>]<br><a href="https://github.com/q1xiangchen/VMPs" style="color:#000000;">Code</a>, <a href="https://time.griffith.edu.au/paper-sites/motion-prompts/" style="color:#000000;">Project website</a>, <a href="https://leiwangr.github.io/files/u7227010_poster.png" style="color:#000000;">Poster</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2024</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E5E4E2">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2405.01461"><strong><span class="gsc_a_at">SATO: Stable Text-to-Motion Framework</span></strong></a>
				<div class="gs_gray">W Chen*, H Xiao*, E Zhang*, L Hu, <strong>L Wang</strong>, M Liu, C Chen</div>
				<div class="gs_gray">ACM Multimedia (<strong>ACM-MM</strong>), 6989 - 6997</div>
			</td>
			<td class="gsc_a_c">(* denotes equal contribution.)<br>[<font color="red"><strong>A*</strong></font>]<br><a href="https://github.com/sato-team/Stable-Text-to-motion-Framework" style="color:#000000;">Code</a>, <a href="https://sato-team.github.io/Stable-Text-to-Motion-Framework/" style="color:#000000;">Project website</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2024</span></td>
		</tr>
		<tr class="gsc_a_tr"> <!--  style="background-color:#E0FFFF" -->
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2310.05615.pdf"><strong><span class="gsc_a_at">Adaptive Multi-head Contrastive Learning</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>$^\dagger$, P Koniusz, T Gedeon, L Zheng</div>
				<div class="gs_gray">European Conference on Computer Vision (<strong>ECCV</strong>), 404-421</div>
			</td>
			<td class="gsc_a_c">[<font color="red"><strong>A*</strong></font>]<br><a href="https://github.com/LeiWangR/cl" style="color:#000000;">Code</a>, <a href="https://leiwangr.github.io/files/eccv24_poster.pdf" style="color:#000000;">Poster </a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2024</span></td>
		</tr>
		<tr class="gsc_a_tr"> 
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2402.03019.pdf"><strong><span class="gsc_a_at">Taylor Videos for Action Recognition</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>$^\dagger$, X Yuan, T Gedeon, L Zheng</div>
				<div class="gs_gray">International Conference on Machine Learning (<strong>ICML</strong>), 52117-52133</div>
			</td>
			<td class="gsc_a_c">[<font color="red"><strong>A*</strong></font>]<br><a href="https://github.com/LeiWangR/video-ar" style="color:#000000;">Code</a>, <a href="https://leiwangr.github.io/files/icml24-poster.pdf" style="color:#000000;">Poster </a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2024</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E0FFFF"> 
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2110.05216.pdf"><strong><span class="gsc_a_at">High-order Tensor Pooling with Attention for Action Recognition</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, K Sun, P Koniusz</div>
				<div class="gs_gray">IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 3885-3889</div>
			</td>
			<td class="gsc_a_c">(An extension of our TPAMI'20)<br>[<font color="red"><strong>B</strong>, oral</font>] <br><a href="https://github.com/LeiWangR/HoTP" style="color:#000000;"> Code (preprocessing)</a><br><a href="https://leiwangr.github.io/files/icassp24_hop_suppl.pdf" style="color:#000000;">Appendix</a>, <a href="https://leiwangr.github.io/files/icassp24_hot_lecture.pdf" style="color:#000000;">Slides</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2023</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E0FFFF"> 
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2310.10059.pdf"><strong><span class="gsc_a_at">Flow Dynamics Correction for Action Recognition</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, P Koniusz</div>
				<div class="gs_gray">IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 3795-3799</div>
			</td>
			<td class="gsc_a_c">[<font color="red"><strong>B</strong></font>]<br> <a href="https://leiwangr.github.io/files/icassp24_hal_poster.pdf" style="color:#000000;">Poster </a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2023</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E0FFFF">
			<td class="gsc_a_t"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_3Mformer_Multi-Order_Multi-Mode_Transformer_for_Skeletal_Action_Recognition_CVPR_2023_paper.pdf"><strong><span class="gsc_a_at">3Mformer: Multi-order Multi-mode Transformer for Skeletal Action Recognition</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, P Koniusz</div>
				<div class="gs_gray">Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 5620-5631</div>
			</td>
			<td class="gsc_a_c">[<font color="red"><strong>A*</strong></font>]<br> <a href="https://leiwangr.github.io/files/cvpr23-poster.pdf" style="color:#000000;">Poster </a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2023</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E0FFFF">
			<td class="gsc_a_t"><a href="https://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Temporal-Viewpoint_Transportation_Plan_for_Skeletal_Few-shot_Action_Recognition_ACCV_2022_paper.pdf"><strong><span class="gsc_a_at">Temporal-Viewpoint Transportation Plan for Skeletal Few-shot Action Recognition</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, P Koniusz</div>
				<div class="gs_gray">Asian Conference on Computer Vision (ACCV), 307-326</div>
			</td>
			<td class="gsc_a_c">[<font color="red"><strong>B</strong>, oral, 4.9% acceptance rate, <br><strong>Best Student Paper Award</strong></font>]<br><a href="https://leiwangr.github.io/files/ACCV2022_Best Student Paper Award.pdf" style="color:#000000;">Award certificate</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2022</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E0FFFF">
			<td class="gsc_a_t"><a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136810174.pdf"><strong><span class="gsc_a_at">Uncertainty-DTW for Time Series and Sequences</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, P Koniusz</div>
				<div class="gs_gray">European Conference on Computer Vision (<strong>ECCV</strong>), 176-195</div>
			</td>
			<td class="gsc_a_c">[<font color="red"><strong>A*</strong>, oral, 2.7% acceptance rate</font>]<br><a href="https://github.com/LeiWangR/uDTW" style="color:#000000;">Code</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2022</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E0FFFF">
			<td class="gsc_a_t"><a href="https://dl.acm.org/doi/10.1145/3474085.3475572"><strong><span class="gsc_a_at">Self-supervising Action Recognition by Statistical Moment and Subspace Descriptors</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, P Koniusz</div>
				<div class="gs_gray">ACM International Conference on Multimedia (<strong>ACM-MM</strong>), 4324-4333</div>
			</td>
			<td class="gsc_a_c">[<strong><font color="red">A*</font></strong>]<br><a href="https://github.com/LeiWangR/ODFSDF" style="color:#000000;">Code</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2021</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E0FFFF">
			<td class="gsc_a_t"><a href="https://ieeexplore.ieee.org/document/9008573"><strong data-bind="text: title">Hallucinating IDT Descriptors and I3D Optical Flow Features for Action Recognition with CNNs</strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, P Koniusz, DQ Huynh</div>
				<div class="gs_gray">IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 8698-8708</div>
			</td>
			<td class="gsc_a_c">[<strong><font color="red">A*</font></strong>]</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2019</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#FFFFE0">
			<td class="gsc_a_t"><a href="https://ieeexplore.ieee.org/document/8803051"><strong><span class="gsc_a_at">Loss Switching Fusion with Similarity Search for Video Classification</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, DQ Huynh, MR Mansour</div>
				<div class="gs_gray">26th IEEE International Conference on Image Processing (ICIP), 974-978</div>
			</td>
			<td class="gsc_a_c">[<strong><font color="red">B</font></strong><font color="red">, industrial research <br>+ 1 AU<strong>&nbsp;patent</strong></font>]<br><a href="https://github.com/LeiWangR/LSFNet" style="color:#000000;">Our dataset</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2019</span></td>
		</tr>
	</tbody>
</table>

<h2>Workshops</h2>

<table id="gsc_a_t">
	<tbody id="gsc_a_b">
		<tr class="gsc_a_tr" style="background-color:#E5E4E2">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2405.10718"><strong><span class="gsc_a_at">SignLLM: Sign Languages Production Large Language Models</span></strong></a>
				<div class="gs_gray">S Fang, C Chen, <strong>L Wang</strong>, C Zheng, C Sui, Y Tian</div>
				<div class="gs_gray">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</div>
			</td>
			<td class="gsc_a_c"><a href="https://signllm.github.io" style="color:#000000;">Project website</a><br><a href="https://github.com/SignLLM/Prompt2Sign" style="color:#000000;">Dataset & Code</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr>
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://dl.acm.org/doi/pdf/10.1145/3701716.3717746"><strong><span class="gsc_a_at">The Journey of Action Recognition</span></strong></a>
				<div class="gs_gray">X Ding, <strong>L Wang</strong>$^\dagger$</div>
				<div class="gs_gray">Companion Proceedings of the ACM Web Conference 2025, 1869-1884</div>
			</td>
			<td class="gsc_a_c">[<font color="red">Oral, <strong>Best Paper Award</strong></font>]<br><a href="https://leiwangr.github.io/files/Transformative Insight Award.pdf" style="color:#000000;">Award certificate</a><br><a href="https://github.com/Darcyddx/Video-Action-Recognition" style="color:#000000;">Code</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr>
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://dl.acm.org/doi/pdf/10.1145/3701716.3717744"><strong><span class="gsc_a_at">Do Language Models Understand Time?</span></strong></a>
				<div class="gs_gray">X Ding, <strong>L Wang</strong>$^\dagger$</div>
				<div class="gs_gray">Companion Proceedings of the ACM Web Conference 2025, 1855-1868</div>
			</td>
			<td class="gsc_a_c"><font color="red">Oral</font> <br> <a href="https://github.com/Darcyddx/Video-LLM" style="color:#000000;">Code</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr>
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://dl.acm.org/doi/pdf/10.1145/3701716.3717739"><strong><span class="gsc_a_at">Evolving Skeletons: Motion Dynamics in Action Recognition</span></strong></a>
				<div class="gs_gray">J Qiu, <strong>L Wang</strong>$^\dagger$</div>
				<div class="gs_gray">Companion Proceedings of the ACM Web Conference 2025, 1916-1937</div>
			</td>
			<td class="gsc_a_c">Poster</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr>
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://dl.acm.org/doi/pdf/10.1145/3701716.3717748"><strong><span class="gsc_a_at">TIME 2025: 1st International Workshop on Transformative Insights in Multi-faceted Evaluation</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>$^\dagger$, MZ Hossain, S Islam, T Gedeon, S Alghowinem, I Yu, S Bono, X Zhu, G Nguyen, NAH Haldar, SMJ Jalali, MA Razzaque, I Razzak, MR Islam, S Uddin, N Janjua, A Krishna, M Ashraf</div>
				<div class="gs_gray">Companion Proceedings of the ACM Web Conference 2025, 1848-1851</div>
			</td>
			<td class="gsc_a_c"><a href="https://time.griffith.edu.au/workshop/time2025/" style="color:#000000;">Workshop website</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr>
	</tbody>
</table>

<!-- <h3>&nbsp;</h3> -->
<h2>Patents</h2>
<table id="gsc_a_t">
	<tbody id="gsc_a_b">
		<tr class="gsc_a_tr" style="background-color:#FFFFE0">
			<td class="gsc_a_t"><a href=""><strong><span class="gsc_a_at">System and Method of Detecting Anomalies from Mass Data</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong></div>
				<div class="gs_gray">US patent (provisional, SN 63/326,525)</div>
			</td>
<!-- 			<td class="gsc_a_c">&nbsp;</td> -->
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2022</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#FFFFE0">
			<td class="gsc_a_t"><a href="http://pericles.ipaustralia.gov.au/ols/auspat/applicationDetails.do?applicationNo=2019903775"><strong><span class="gsc_a_at">Method and System for Classifying Video Data</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, G Woods</div>
				<div class="gs_gray">AU Patent AU 2,019,903,775</div>
			</td>
<!-- 			<td class="gsc_a_c">&nbsp;</td> -->
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2019</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#FFFFE0">
			<td class="gsc_a_t"><a href="http://pericles.ipaustralia.gov.au/ols/auspat/applicationDetails.do?applicationNo=2019900316"><strong><span class="gsc_a_at">System and Method of Video Data Retrieval</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, M Reda Mansour, G Woods</div>
				<div class="gs_gray">AU Patent AU 2,019,900,316</div>
			</td>
<!-- 			<td class="gsc_a_c">&nbsp;</td> -->
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2019</span></td>
		</tr>
	</tbody>
</table>
<h2>Theses</h2>
<table id="gsc_a_t">
	<tbody id="gsc_a_b">
		<tr class="gsc_a_tr" style="background-color:#FFE4E1">
			<td class="gsc_a_t"><a href="https://openresearch-repository.anu.edu.au/bitstream/1885/301236/1/ANU_PhD_Thesis_corrected.pdf"><strong><span class="gsc_a_at">Robust Human Action Modelling</span></strong></a><br />
				<div class="gs_gray"><strong>L Wang</strong></div>
				<div class="gs_gray">PhD thesis<font color="blue">*</font>, The Australian National University</div>
			</td>
<!-- 			<td class="gsc_a_c">&nbsp;</td> -->
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">Nov 2023</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#FFE4E1">
			<!-- https://www.researchgate.net/profile/Lei_Wang51/publication/332369012_Analysis_and_Evaluation_of_Kinect-based_Action_Recognition_Algorithms/links/5cb001be4585156cd7916b18/Analysis-and-Evaluation-of-Kinect-based-Action-Recognition-Algorithms.pdf -->
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2112.08626.pdf"><strong><span class="gsc_a_at">Analysis and Evaluation of Kinect-based Action Recognition Algorithms</span></strong></a><br />
				<div class="gs_gray"><strong>L Wang</strong></div>
				<div class="gs_gray">Master&rsquo;s thesis, The University of Western Australia</div>
			</td>
<!-- 			<td class="gsc_a_c">&nbsp;</td> -->
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">Nov 2017</span></td>
		</tr>
	</tbody>
</table>
<p style="font-family:Arial; font-size: 12px; color: blue">
*: I would like to extend my heartfelt gratitude to three esteemed PhD thesis examiners, namely <a href="https://lingqiao-adelaide.github.io/lingqiaoliu.github.io/">Dr. Lingqiao Liu</a> (University of Adelaide) and <a href="https://wp8619.github.io/">Dr. Peng Wang</a> (University of Electronic Science and Technology of China), as well as an anonymous examiner, for their invaluable insights, meticulous examination, and constructive feedback on my research work.
<br>
*: The 1st chapter of my thesis is an invited talk ("Action Recognition: Past, Present and Future") for 'The Next Generation of International Chinese Young Students Face to Face', Issue 21 of the International Cooperation and Exchange Program Series Activities at Harbin Institute of Technology (Shenzhen). Chapters 2 through 8 comprise published works presented at TIP'20, ICCV'19, ACMMM'21, TPAMI'20, CVPR'23, ECCV'22 (oral), and ACCV'22 (oral, recipient of the Best Student Paper Award). Chapter 9 provides a summary and outlines future work. Other works completed during my PhD, such as contributions to the IJCV special issue on ACCV'22 and presentations at ICASSP (x2, 1 oral), are not included in this thesis.
</p>
 
<!-- <p>&nbsp;</p> -->
<h2>arXiv preprints</h2>
<!-- Below is a list of the <em>currently available</em> arXiv preprints. -->

Please note that arXiv papers have not been peer-reviewed and should be considered as preliminary versions of research, subject to revision upon formal review and publication.

<table id="gsc_a_t">
	<tbody id="gsc_a_b">
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2205.02071"><strong><span class="gsc_a_at">Representation-Centric Survey of Skeletal Action Recognition and the ANUBIS Benchmark</span></strong></a>
				<div class="gs_gray">Y Liu, J Yang, M Perera, P Ji, D Kim, M Xu, T Wang,
S Anwar, T Gedeon, <strong>L Wang</strong>$^\dagger$, Z Qin</div>
				<div class="gs_gray">arXiv preprint arXiv:2205.02071</div>
			</td>
			<td class="gsc_a_c">Jiyao Yang conducted this research under the supervision of Lei Wang, Zhenyue Qin, and Yang Liu. This work presents a systematic review of skeleton-based action recognition methods and the construction of a new large-scale benchmark dataset, aiming to advance robustness and generalization in complex real-world scenarios. [<a href="https://yliu1082.github.io/ANUBIS/" style="color:#000000;">Project website</a>][<a href="https://github.com/yliu1082/ANUBIS-Sourcecode" style="color:#000000;">Code</a>]</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr>
<!-- 		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href=""><strong><span class="gsc_a_at">NA</span></strong></a>
				<div class="gs_gray">EK Tenagyei, <strong>L Wang</strong>$^\dagger$, J Zhou, Y Gao, P Koniusz</div>
				<div class="gs_gray">NA</div>
			</td>
			<td class="gsc_a_c">Tenagyei Edwin Kwadwo conducted this work under the supervision of Lei Wang, as part of his PhD research training at at the TIME Lab, Griffith University (TIME@ARC Hub & Griffith). This work was supported by computational resources provided by the Australian Government through the National Computational Infrastructure (NCI) under the CSIRO Allocation Scheme. <font color="blue">This paper is currently not publicly available.</font></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr> -->
<!-- 		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href=""><strong><span class="gsc_a_at">NA</span></strong></a>
				<div class="gs_gray">X Ding, <strong>L Wang</strong>$^\dagger$, P Koniusz, Y Gao</div>
				<div class="gs_gray">NA</div>
			</td>
			<td class="gsc_a_c">Xi Ding conducted this work under the supervision of Lei Wang, as part of his role as an ARC Hub Research Scholar at Griffith University (TIME@ARC Hub & Griffith). This work was supported by computational resources provided by the Australian Government through the National Computational Infrastructure (NCI) under the CSIRO Allocation Scheme. <font color="blue">This paper is currently not publicly available.</font></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr> -->
<!-- 		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href=""><strong><span class="gsc_a_at">NA</span></strong></a>
				<div class="gs_gray">W Diao, <strong>L Wang</strong>$^\dagger$, A Busch, J Zhou, Y Gao</div>
				<div class="gs_gray">NA</div>
			</td>
			<td class="gsc_a_c">Wenxiang Diao conducted this work under the supervision of Lei Wang as part of his role as a Research Intern at the TIME Lab, Griffith University (TIME@ARC Hub & Griffith). This work was supported by the National Computational Merit Allocation Scheme 2025 (NCMAS 2025), with computational resources provided by NCI Australia, an NCRIS-enabled capability supported by the Australian Government. <font color="blue">This paper is currently not publicly available.</font></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr> -->
<!-- 		<tr class="gsc_a_tr" style="background-color:#E5E4E2">
			<td class="gsc_a_t"><a href=""><strong><span class="gsc_a_at">NA</span></strong></a>
				<div class="gs_gray">H Jia, W Chen, Z Huang, <strong>L Wang</strong>, H Xiao, N Jia, K Wu, S Lai, B Tian, Y Yue</div>
				<div class="gs_gray">NA</div>
			</td>
			<td class="gsc_a_c">Wenshuo Chen will begin his PhD studies in Fall 2025 at the Hong Kong University of Science and Technology (HKUST Guangzhou), under the supervision of Associate Professor Yutao Yue. <font color="blue">This paper is currently not publicly available.</font> </td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr> -->
<!-- 		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://arxiv.org/abs/2505.16702"><strong><span class="gsc_a_at">Truth and Trust: Fake News Detection via Biosignals</span></strong></a>
				<div class="gs_gray">G Nguyen, <strong>L Wang</strong>, Y Jiang, T Gedeon</div>
				<div class="gs_gray">arXiv preprint arXiv:2505.16702</div>
			</td>
			<td class="gsc_a_c">Gennie Nguyen conducted this research under the supervision of Lei Wang and Tom Gedeon as part of her final year master's research project at ANU. </td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr> -->
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://arxiv.org/abs/2505.16730"><strong><span class="gsc_a_at">Detecting Fake News Belief via Skin and Blood Flow Signals</span></strong></a>
				<div class="gs_gray">G Nguyen, <strong>L Wang</strong>, Y Jiang, T Gedeon</div>
				<div class="gs_gray">arXiv preprint arXiv:2505.16730</div>
			</td>
			<td class="gsc_a_c">Gennie Nguyen conducted this research under the supervision of Lei Wang and Tom Gedeon as part of her final year master's research project at ANU. </td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr>
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://arxiv.org/abs/2505.12433"><strong><span class="gsc_a_at">SRLoRA: Subspace Recomposition in Low-Rank Adaptation via Importance-Based Fusion and Reinitialization</span></strong></a>
				<div class="gs_gray">H Yang, <strong>L Wang</strong>$^\dagger$, MZ Hossain</div>
				<div class="gs_gray">arXiv preprint arXiv:2505.12433</div>
			</td>
			<td class="gsc_a_c">Haodong Yang conducted this research under the supervision of Lei Wang and Md Zakir Hossain as part of his final year master's research project at ANU. This work was supported by computational resources provided by the Pawsey Supercomputing Centre, a high-performance computing facility funded by the Australian Government.</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr>
<!-- 		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href=""><strong><span class="gsc_a_at">NA</span></strong></a>
				<div class="gs_gray">X Ding, <strong>L Wang</strong>$^\dagger$, P Koniusz, Y Gao</div>
				<div class="gs_gray">NA</div>
			</td>
			<td class="gsc_a_c">Xi Ding conducted this work under the supervision of Lei Wang, as part of his role as an ARC Hub Research Scholar at Griffith University (TIME@ARC Hub & Griffith). This work was supported by computational resources provided by the Australian Government through the National Computational Infrastructure (NCI) under the ANU Merit Allocation Scheme. Additional support was provided through the NCI CSIRO Allocation Scheme. <font color="blue">This paper is currently not publicly available.</font></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr> -->
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2506.23863"><strong><span class="gsc_a_at">Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction
</span></strong></a>
				<div class="gs_gray">J Ma, <strong>L Wang</strong>, M Liu, D Ahmedt-Aristizabal, C Nguyen</div>
				<div class="gs_gray">arXiv preprint arXiv:2506.23863</div>
			</td>
			<td class="gsc_a_c">Jiahao Ma is a PhD candidate at the Australian National University (ANU) and Data61/CSIRO, formally supervised by Miaomiao Liu, David Ahmedt Aristizabal, and Chuong Nguyen. Lei Wang provides informal co-supervision, offering ongoing academic guidance and support throughout the project. [<a href="https://github.com/Jiahao-Ma/puzzles-code" style="color:#000000;">Code</a>, <a href="https://jiahao-ma.github.io/puzzles/" style="color:#000000;">Project website</a>]</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr>
<!-- 		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href=""><strong><span class="gsc_a_at">NA</span></strong></a>
				<div class="gs_gray">X Ding, <strong>L Wang</strong>$^\dagger$, Y Gao, P Koniusz</div>
				<div class="gs_gray">NA</div>
			</td>
			<td class="gsc_a_c">Xi Ding conducted this work under the supervision of Lei Wang, as part of his role as an ARC Hub Research Scholar at Griffith University (TIME@ARC Hub & Griffith). This work was supported by computational resources provided by the Australian Government through the National Computational Infrastructure (NCI) under the ANU Merit Allocation Scheme. Additional support was provided through the NCI CSIRO Allocation Scheme. <font color="blue">This paper is currently not publicly available.</font></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2025</span></td>
		</tr> -->
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2412.18298"><strong><span class="gsc_a_at">Quo Vadis, Anomaly Detection? LLMs and VLMs in the Spotlight</span></strong></a>
				<div class="gs_gray">X Ding, <strong>L Wang</strong>$^\dagger$</div>
				<div class="gs_gray">arXiv preprint arXiv:2412.18298</div>
			</td>
			<td class="gsc_a_c">Xi Ding, a Research Assistant with the Temporal Intelligence and Motion Extraction (TIME) Lab at ANU, contributed to this work. TIME Lab is a dynamic research team comprising master’s and honours students focused on advancing video processing and motion analysis. This research was conducted under the supervision of Lei Wang. [<a href="https://github.com/Darcyddx/VAD-LLM" style="color:#000000;">Code</a>]</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2024</span></td>
		</tr>
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2411.15284"><strong><span class="gsc_a_at">When Spatial meets Temporal in Action Recognition</span></strong></a>
				<div class="gs_gray">H Chen, <strong>L Wang</strong>$^\dagger$, Y Chen, T Gedeon, P Koniusz</div>
				<div class="gs_gray">arXiv preprint arXiv:2411.15284</div>
			</td>
			<td class="gsc_a_c">Huilin Chen conducted this research under the supervision of Lei Wang for her final year honors research project at ANU. This work was supported by the National Computational Merit Allocation Scheme 2024 (NCMAS 2024), with computational resources provided by NCI Australia, an NCRIS-enabled capability supported by the Australian Government.</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2024</span></td>
		</tr>
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2404.13016.pdf"><strong><span class="gsc_a_at">Optimizing Calibration by Gaining Aware of Prediction Correctness</span></strong></a>
				<div class="gs_gray">Y Liu, <strong>L Wang</strong>, Y Zou, J Zou, L Zheng</div>
				<div class="gs_gray">arXiv preprint arXiv:2404.13016</div>
			</td>
			<td class="gsc_a_c">Research report. [<a href="https://github.com/liuyvchi/Correctness-aware-calibration" style="color:#000000;">Code</a>]</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2024</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E0FFFF">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2112.12668.pdf"><strong><span class="gsc_a_at">3D Skeleton-based Few-shot Action Recognition with JEANIE is not so Naïve</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, J Liu, P Koniusz</div>
				<div class="gs_gray">arXiv preprint arXiv:2112.12668</div>
			</td>
			<td class="gsc_a_c">An extended version has been accepted by ACCV'22 [oral] and has been awarded the Sang Uk Lee Best Student Paper Award. The further extension of ACCV'22 has been accepted for publication by the IJCV special issue.</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2021</span></td>
		</tr>
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2309.15768.pdf"><strong><span class="gsc_a_at">AI in Software Engineering: Case Studies and Prospects</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong></div>
				<div class="gs_gray">arXiv preprint arXiv:2309.15768</div>
			</td>
			<td class="gsc_a_c">Technical Report. The author conducted this work while enrolled as a master's student at UWA, specifically for the CITS5502 Software Processes unit in 2017.</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2017</span></td>
		</tr>
	</tbody>
</table>
<p>&nbsp;</p>

<font size="3" color="blue">
	Light cyan highlights the research works during my PhD candidature and light yellow highlights my industrial research works.
	My theses are in misty rose color and other collaborative research works are in platinum color.
	Last updated: 04/05/2024.	
</font>
