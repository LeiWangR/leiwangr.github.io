---
layout: archive
title: "Projects"
permalink: /projects/
author_profile: true
---

<style>
a:link {
  text-decoration: none;
}

a:visited {
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

a:active {
  text-decoration: underline;
}
</style>

<!-- Skills
====== -->

<!-- * Programming languages
  * Python
  * Matlab
  * C/C++
  * LATEX
  * Unix shell scripts

* Machine learning libraries
  * Pytorch
  * Scikit-learn
  * TensorFlow
  * MatConvNet -->



<!-- * Action recognition
* Anomaly detection
* Video image processing
* One- & few-shot learning
* Deep learning
* Tensor learning
* Domain adaptation -->
<!-- <p>&nbsp;</p> -->
<!-- <h2>Research project</h2> -->

<!-- I am working on the following projects: -->

<!-- * Automatic Training Data Search and Model Evaluation by Measuring Domain Gap -->

<!-- * Human action recognition in videos -->

<!-- * Industrial research project -->
<!--   * Human-related anomaly detection in surveillance videos -->
<!--   * Research student: [Supasun Khumpraphan](https://micsupasun.github.io/) -->

<!-- I always look for motivated and hardworking students who are interested in doing research with me. However, applicants must possess good mathematical and computational skills, preferably with prior experience in machine learning and/or computer vision. Strong programming skills in Python, Matlab, and/or TensorFlow are essential, and hands-on deep learning expertise is important as well. Additionally, integrity, honesty, the ability to work under pressure, and the capacity to explore while adhering to strict directions are all essential qualities. -->

Please visit my [TIME Lab project page](https://time.anu.edu.au/about/) for more details.

<!-- <font color="red">The TIME Lab has announced that it will not be accepting new final-year research projects for the upcoming semester. Students are encouraged to explore alternative opportunities for their projects. Special consideration requests must first be approved by Professor Liang Zheng and Professor Michael Norrish, with final registration contingent upon approval from Lei Wang. When submitting requests, students are asked to ensure all relevant parties are CC'd in the email correspondence.</font> -->

<!-- <h2>TIME Lab announces criteria for inaugural medal</h2>

The TIME Lab has officially revealed the selection criteria for its prestigious inaugural medal, which will recognize exceptional honours and master's students from ANU. Six students, currently completing their final-year, 24-unit research projects under TIME Lab supervision, are in contention. This highly coveted award will be presented to the top-ranking student who demonstrates an outstanding balance between life, study, work, and research. 

TIME Lab 2024 medal criteria:

- Research (40% total):
  - Examiner reports highlight significant contributions and commend the work (20%)
  - Skills developed throughout the research process (15%)
  - Publication output (a minor focus in this first year, 5%)

- Life balance and personal growth (40% total):
  - Achieving a well-balanced life between work, study, and personal pursuits (20%)
  - Lessons learned, with deep insights into all aspects of life (15%)
  - Displaying positivity, hope, courage, and thoughtfulness (5%)

- Contributions (20% total):
  - To TIME Lab (10%): Service to the lab, including responsibilities such as planning, designing, delivering, and negotiating
  - To the broader community (10%): Impact on the ANU or wider community, including addressing industry challenges or offering real-world solutions. -->


<h2>Student Project</h2>

<h3>Project 1: Bachelor & Honours Research Project <font color="red">[Ongoing]</font></h3>

I am a [registered ANU Higher Degree Research (HDR) supervisor](https://services.anu.edu.au/research-support/hdr-supervision-development/hdr-supervision-registration-and-renewal/hdr-1) (registration is valid until 31 December 2025).

If you are an ANU undergraduate seeking an Honors research project, please note that I typically accept a maximum of two Honors students each year to ensure high-quality supervision, and I prefer 24-unit projects.

If you are an ANU master's student seeking individual projects, I require candidates to have a solid research background, a strong GPA, and proficient coding skills. Additionally, it is important that you have adequate time available for research.

* Selected posters from former master’s, honours, and internship students (**primary supervision**):
  * Dexuan Ding (1 x <strong>arXiv preprint</strong>, [Poster](https://leiwangr.github.io/files/u7321641_poster.png)), *Honours Student*, 2024/02/19 – 2024/10/25 
  * [Qixiang Chen](https://q1xiangchen.github.io/) (1 x <font color="red"><strong>ACML'24</strong> oral</font>, [Poster](https://leiwangr.github.io/files/u7227010_poster.png)), *Honours Student*, 2024/02/19 – 2024/10/25 
  * Huilin Chen (1 x <strong>arXiv preprint</strong>, [Poster](https://leiwangr.github.io/files/u7326198_poster.png)), *Honours Student*, 2024/02/19 – 2024/10/25 
  * [Liyun Zhu](https://tom-roujiang.github.io/liyun_zhu/) (1 x <font color="red"><strong>NeurIPS'24</strong></font>, [Poster](https://leiwangr.github.io/files/u7778917_poster.png)), *Master's Student*, 2024/02/19 – 2024/10/25 
  * [Xiuyuan Yuan](https://jackyuanx.github.io/) (1 x <font color="red"><strong>ICML'24</strong></font>, [Poster](https://leiwangr.github.io/files/icml24-poster.pdf)), *Summer Research Internship*, 2023/11/20 – 2024/02/02


<h3>Project 2: ANU TechLauncher - TIME Lab’s TIME Space <font color="red">[Proposed for 2025 S1 & S2]</font></h3>

[TIME Lab](https://time.anu.edu.au/) is a dynamic team of honours and master's students working on final-year research projects under the supervision of Lei Wang. Over the past year, the team has made significant strides, not only in research but also in achieving a healthy work-life balance. To celebrate this progress, TIME Lab aims to design a website, TIME Space, to showcase the memorable moments of the past year. TIME Space will be a fully interactive platform where photos loop endlessly, following the dynamic motion of the TIME Lab logo. Users will be able to engage with the space by selecting specific time frames, locations, or individuals involved in the memories, creating a personalized experience that brings the past year to life.

By undertaking this project, students will gain hands-on experience in full-stack development, designing and deploying a dynamic web application with advanced UX/UI features. They will enhance their skills in frontend and backend development, database management, and cloud deployment, while applying machine learning techniques for smart tagging and image processing. The project challenges students to solve real-world problems creatively, preparing them for professional environments and adding a portfolio-ready achievement to showcase their technical expertise.

This project combines cutting-edge technologies with a meaningful goal: bringing memories to life through an innovative, interactive platform. Students will work in a collaborative, supportive environment, tackling challenges across web development, data visualization, and user interaction. With immediate application in the TIME Lab, the project provides a visible, impactful outcome, offering both personal and professional fulfillment while building critical, in-demand skills.

Students undertaking this project will receive mentorship TIME Lab researchers, along with support from the ANU TechLauncher program, including access to workshops, industry connections, and project management tools. They will work in a collaborative environment, benefiting from feedback from peers and stakeholders. Key resources like cloud services, development platforms, and software tools for web development, machine learning, and UX/UI design will be provided. Regular technical guidance, testing support, and workshops on essential technologies will ensure students have everything needed to succeed.

<h3>Project 3: ANU TechLauncher - Active Vision project <font color="red">[Ongoing]</font></h3>

The [TechLauncher program](https://comp.anu.edu.au/TechLauncher/) is for team-based activities at ANU, where the project and/or group activity undertaken is real, not synthetic. In TechLauncher you will be a team member on a project, and shall be supported by a program of tutorials at which your peers, along with experienced professional tutors/mentors, facilitate your professional development and mediate your team processes and interactions.

If you would like to join a Techlauncher cohort, you can do so by enrolling in any of the following courses: COMP3500 - Software Engineering Project, COMP3550 - Advanced Computing R&D Project, COMP3710 - Topics in Computer Science, COMP4500 - Software Engineering Practice, and COMP8715 - Computing Project.

In the 2024 Semester 2 and 2025 Semester 1, my proposed project is '*Active Vision*'. Francis Williams and Arjun Raj (acting on behalf of Lei Wang) are the clients.

The Active Vision project aims to develop an advanced computer vision system that analyzes the dynamics of tennis swings. Using cutting-edge machine learning techniques, this project focuses on capturing and analyzing motion data to provide players and coaches with detailed insights into swing mechanics. The system leverages high-speed cameras and advanced algorithms to track, interpret, and visualize the movements of a player's body and racket, enabling precise performance assessment and improvement.

Objectives include: (i) Develop analytical algorithms: Create robust algorithms capable of analyzing the captured motion data to identify key performance indicators (KPIs) such as swing speed, angle, and impact accuracy. (ii) Real-time feedback: Implement a real-time feedback system that provides immediate insights and suggestions to players, enhancing their ability to correct and refine their techniques on the spot. (iii) User-friendly interface: Develop an intuitive interface that allows users to easily interact with the system, view their performance data, and track their progress over time.

<a href="https://leiwangr.github.io/files/24-S2-2-Active-Vision.png" style="color: blue">[2024 S2 Poster]</a>

<h3>Project 4: ANU TechLauncher - CAM FALL project <font color="red">[Ongoing]</font></h3>

In the 2024 Semester 1 and 2, my proposed project is '*ActiveIntelligence: CAM-FALL (Camera-based Fall Alert System)*'. This project marks the 2nd Annual Active Intelligence Research Challenge (refer to Industry Research Sponsorship below). Francis and I are the clients.

Step into a safer future with ActiveIntelligence's CAM-FALL system. This innovative project offers a unique opportunity to address a pressing concern using cutting-edge technology. By harnessing the power of cameras, we aim to create a solution that not only identifies falls but also provides prompt assistance, ensuring the well-being of individuals within our community. Participation in the project will provide students with a holistic learning experience encompassing technical skills, problem-solving abilities, project management competencies, and ethical awareness, preparing them for future endeavors in the fields of computer vision, artificial intelligence, and software engineering.

<a href="https://leiwangr.github.io/files/24-S1-2-C-CAMFALL.png" style="color: blue">[2024 S1 Poster]</a> <a href="https://leiwangr.github.io/files/24-S2-2-C-CAMFALL.png" style="color: blue">[2024 S2 Poster]</a>

<!--[Project Goals]

1. Develop a robust fall detection algorithm using computer vision techniques for real-time analysis of camera footage.

2. Implement audio localization capabilities to enhance fall detection accuracy and optimize camera positioning.

3. Integrate pan-tilt-zoom (PTZ) functionality to dynamically adjust the camera's orientation towards the detected fall location.

4. Design an alert system to promptly notify caregivers or emergency contacts upon fall detection, accompanied by video footage for further assessment.

5. Ensure user-friendly adaptability to various home environments, considering factors such as lighting conditions and camera placement.

[Technical Details]

1. Primary programming languages include Python, with utilization of OpenCV libraries for computer vision tasks.

2. Implementation of audio processing techniques to localize sound sources and aid in fall detection.

3. Hardware components encompass IP cameras featuring PTZ capabilities and microphones for audio capture.

4. The system architecture is designed to operate on either a local server or a cloud platform, contingent on scalability and resource demands.

[Project Milestones]

Week 3: Complete preliminary research, conduct feasibility study, define project requirements, and establish development environment.

Week 6: Begin implementation of basic fall detection algorithm using computer vision techniques, and integrate audio localization capabilities.

Week 10: Finalize system functionality with PTZ camera control, implement the alert system, and perform comprehensive testing and validation.

[Expectations]

- Proficiency in advanced computer vision and audio processing techniques is essential for project execution.

- While ambitious, the project's scope aligns with available time and resources. However, achieving optimal performance may necessitate multiple semesters.

- Potential costs encompass licensing fees for software libraries and procurement of hardware such as IP cameras and microphones.

Participation in the project will provide students with a holistic learning experience encompassing technical skills, problem-solving abilities, project management competencies, and ethical awareness, preparing them for future endeavors in the fields of computer vision, artificial intelligence, and software engineering.


- Computer Vision: Understanding of computer vision principles and techniques is essential for developing robust fall detection algorithms. Proficiency in libraries such as OpenCV for image processing and analysis is necessary.

- Audio Processing: Knowledge of audio processing techniques will be required to implement sound localization capabilities for enhancing fall detection accuracy. Familiarity with signal processing algorithms and libraries may be necessary.

- Programming: Strong programming skills, particularly in Python, are necessary for implementing the system's algorithms and functionalities. Knowledge of other relevant languages and frameworks may also be beneficial.

- Machine Learning: Familiarity with machine learning techniques, such as supervised and unsupervised learning, may be beneficial for improving the accuracy of fall detection algorithms.

- Hardware Integration: Understanding of hardware components, such as IP cameras with PTZ capabilities and microphones, is necessary for integrating these devices into the system effectively.

- System Architecture: Knowledge of system architecture principles is required to design and implement a scalable and efficient system architecture. Understanding of cloud platforms and server-client architectures may be beneficial.

- Software Development Lifecycle: Familiarity with software development lifecycle processes, including requirements analysis, design, implementation, testing, and deployment, is essential for managing the project effectively.

- Problem-solving Skills: Ability to troubleshoot and solve complex technical problems that may arise during the development process is crucial for ensuring project success.

- Communication: Strong communication skills are necessary for effectively collaborating with team members, stakeholders, and potential end-users throughout the project lifecycle.

- Time Management: Effective time management skills are essential for meeting project milestones and deadlines within the allocated timeframe.

- Ethical Considerations: Awareness of ethical considerations related to privacy, data security, and responsible use of technology is necessary for ensuring the system aligns with ethical standards and regulations. -->


<h3>Project 5: Industry Research Sponsorship for Students <font color="red">[Completed]</font></h3>

I annually organize multiple industry research awards to support promising research candidates (Bachelor's, Honours, and Master's Students) interested in my research areas. Please note that only students from the Group of Eight (Go8) Australian universities are considered.

*The Active Intelligence Research Challenge Award* was established by Mr. Lei Wang and Mr. Francis Williams to honor individuals who have demonstrated exceptional dedication, expertise, and a commitment to excellence in the field of Artificial Intelligence and Machine Learning. The Award includes a cash prize (up to $5,000) along with a research internship (6-12 months full-time). These awards are provided by Active Intelligence Corporation LLC.

The Awards are granted solely on merit and are not subject to quotas. Typically, four candidates can receive awards from a pool of 20 candidates (with a minimum GPA of 6.75/7), resulting in an approximate acceptance rate of 20%, based on the 2023-2024 results. This year marks the 1st Annual Active Intelligence Research Challenge.

For more information regarding the application and award process, please contact the awarded candidates before reaching out to me.

- <font color="blue"> 2023-2024 awarded ANU international students are:</font>
  -  [Mr. Arjun Raj](https://arjunraj.com/) <font color="blue"> (Bachelor of Advanced Computing (Research and Development) (Honours)),</font>
  -  [Mr. Liyun Zhu](https://tom-roujiang.github.io/liyun_zhu/) <font color="blue"> (Master of Machine Learning and Computer Vision),</font>
  -  [Mr. Liwen Luo](https://luoshanji99.github.io/) <font color="blue"> (Master of Machine Learning and Computer Vision), and </font>
  -  [Mr. Qixiang Chen](https://q1xiangchen.github.io/) <font color="blue"> (Bachelor of Advanced Computing (Honours)). </font>

<!-- - <font color="blue"> Visiting student: </font>
  - <font color="blue"> Mr. Zerui Wang (The University of Western Australia, Bachelor of Science), 16/06/2023 - 19/07/2023.</font> -->



<h3>Project 5: ANU Summer Research Scholars Program <font color="red">[Completed]</font></h3>

ANU offers Summer Research Scholarships (SRS) and Summer Research Internships (SRI). External (non-ANU) students from Australia and New Zealand are eligible to apply for a Summer Research Scholarship. Current ANU students are eligible to apply for a Summer Research Internship. 

The SRS scholarship includes board and on-campus accommodation for 9 weeks (run between Monday 20 Nov 2023 to Friday 19 Jan 2024), including the Christmas shutdown period. Candidates are awarded a stipend of \\$150 per week, along with a travel allowance of up to \\$1,000 for students relocating from interstate or from New Zealand. The SRI scholarship provides a stipend of \\$400 per week to assist with the cost of living during the program. All students participating in the program will be invited to a variety of shared social events, workshops and seminars during the summer period.

For more details, please refer to [Summer Research Scholarships](https://cecc.anu.edu.au/current-students/research-opportunities/summer-research-scholarships). You can find my research projects by visiting [2023-2024 ANU CECC SRS Projects Page](https://cecc.anu.edu.au/current-students/research-opportunities/summer-research-projects-2023), get in touch if you are interested.

- <font color="blue"> 2023-2024 awarded ANU domestic student: </font>
  - [Mr. Xiuyuan (Jack) Yuan](https://jackyuanx.github.io) <font color="blue"> (Bachelor of Advanced Computing). </font> Thanks to the ANU School of Computing for providing research funding.


<!-- * Project 1: Video dynamics distillation -->

<!--  Video captures motions such as natural dynamics and human actions. Various research works have been dedicated in learning and extracting spatio-temporal features from videos for human action recognition, anomaly detection, and scene understanding tasks. 
Nowadays, video data have been pre-processed to a variety of data formats using either machine learning tools / computer vision algorithms or physical sensors (e.g., Microsoft Kinect camera with OpenNI framework), for example, optical flows that highlight the video dynamics, depth videos that segment the
foreground objects and human subjects, and skeleton sequences that focus on human poses evolving in time, for better video understanding. 
However, videos contain very redundant information, which hinders the effectiveness and efficiency in extracting the motions of interest, and forces the computer vision community works hard in various large-scale pre-training for downstream video processing tasks. 
Until now, video processing still highly relies on 3D CNNs, and it still suffers from the darkness of large and complicated models. In this research project, we aim to develop more lightweight video data formats which are able to efficiently distill the motions at different granular levels by removing redundant information while focusing on the core dynamics. 
We also explore the acceleration of training and testing while reducing the amount of video data for storage, and answer a scientific question `How much information is contained in the video data and in what format?'.
Many downstream video processing tasks will benefit from our video dynamic distillation process, towards making video understanding much easier and more efficient. This also opens up a new research direction in exploring better video data representations for more lightweight cutting-edge video models. -->

<!-- * Project 2: Training video data optimisation -->

<!--  We live in a world where most of our actions are constantly captured by cameras. Video cameras are spread almost everywhere: in smartphones, computers, drones, surveillance systems, cars, robots, intercoms, etc.
The quality of video data highly affects the performance of video models in its massive useful applications.
Video data optimisation aims to improve the quality of video data being feed into the model for training hence improving the model performance, e.g., recognition accuracy and generalisation abilities. 
However, videos are more challenging due to the extra introduced temporal signals, which makes video understanding hard. 
Compared to image domain, videos suffer from much more serious data quality issues. 
Moving cameras, challenging natural dynamics e.g., rainy, snowy, reflection, etc., background dynamics e.g., tree waving, camera noise and camera shaking issues degrade the video model performance when trained on these unstable and noisy videos. 
Further challenging issues including the label errors and noisy ground truths, and the labeling process for video contents is even more labor-intensive and tedious.
Many video data optimisation methods have been proposed in the literature for training robust video models, for example, video data augmentation is widely used in video model training. Generic video augmentation uses some basic video transformations such as geometric, color space, temporal, erasing and mixing, e.g., video data augmentation via temporal cropping. However, these augmentation methods are still unable to address the video data quality issues as they simply create a large number of diverse video contents with different focuses which still highly rely on the quality of original video data.
In this research project, we aim to explore video data optimisation techniques that are able to improve the model performance for downstream video processing tasks such as human action recognition and anomaly detection. We also explore (i) the influence of training videos on the prediction of a test video (ii) how each individual training video affect the generalisation ability of a model and (iii) do we need all the video data in the training set or shall we drop unfavorable video samples and how. -->


<h2>Research Grant / Funding</h2>

* National Computational Merit Allocation Scheme 2024 (NCMAS 2024)
  * I am a **Chief Investigator (CI)** / **Delegated lead CI** (2024/01/01-2024/12/31)
  * Project title: MotionNetLite: Video Dynamics Distillation for Scalable Models
  * In this research project, we aim to develop more lightweight video data formats which are able to efficiently distill the motions at different granular levels by removing redundant information while focusing on the core dynamics. We also explore the acceleration of training and testing while reducing the amount of video data for storage, and answer a scientific question ‘How much information is contained in the video data and in what format?’. Many downstream video processing tasks will benefit from our video dynamic distillation process, towards making video understanding much easier and more efficient. This also opens up a new research direction in exploring better video data representations for more lightweight cutting-edge video models.
  * This work has potential to impact Safety and Security, Future Cities, IoT, Agri-business, Defence via applications in Health and wellbeing, Safety, and Innovative industries. Our work focuses on researching advanced technologies from data that support all areas of science and society to provide national benefit. Video understanding, e.g., action recognition and anomaly detection, is needed in surveillance of airports, malls, etc. It has applications in monitoring health and well-being of elderly population, in farming, and analysis of crops. This project has also potential to 'Shape Societal Transformations'. For instance, action recognition is a necessary component in recognition from wearable clothing, monitoring health and exercise regimes in the gym, recommendation systems via wearables, recognition of fake videos on social media, etc.
  * This project focuses on 'Analysing, Representing and Modelling data', as video processing models require spatio-temporal modeling of time series, video frames, sequences, etc. My proposal aims at overcoming  'Fundamental limits of data', e.g. by learning in-the-wild and reinforcement learning to explore natural sources of information (e.g. predicting future evolution of video frames learns intrinsic manifold of video/motion data). I hope to bring my ideas to the social media, wearable devices, and recommender systems thus shaping 'data-driven society'. 

* The NCI National AI Flagship Merit Allocation Scheme
  * I am a **Lead Chief Investigator (Lead CI)** (2024/01/01-2024/06/30)
  * Project Title: Robust anomaly detection in human-centric videos
  * This project aims at developing advanced computer vision and deep learning techniques to identify and characterise anomalies in video data where humans are central. The project leverages cutting-edge technology to enhance security, safety, and surveillance systems, making them more effective in detecting unusual behaviours and events, which may range from security breaches and accidents to rare medical conditions in healthcare applications.
  * The significance of this project lies in its unique focus on human-centric videos. While anomaly detection in videos is an established field, the novelty emerges from its specialised applications in situations where human activity is central. The innovative aspects include (i) Robustness: The project seeks to develop highly reliable models capable of detecting anomalies in complex, real-world scenarios, where human interactions and activities can vary significantly. (ii) Real-time analysis: By applying these methods to real-time video streams, the project addresses the demand for timely responses to anomalies in security, industrial, and healthcare settings. (iii) Ethical considerations: The project incorporates ethical considerations, such as ensuring privacy and avoiding bias in the identification of anomalies, thereby making the technology responsible and trustworthy.
  * Beyond its academic contributions, this project has the potential to make significant contributions to the economy, society, environment, and culture.

* Research grant: Review of Xailient’s technical pipeline of facial recognition
  * I am a **Co-Investigator (Co-I)** (**AU$50,815**, 2023/10/30‐2023/12/22)
  * Personnel: Assoc. Prof. Liang Zheng and I are the main personnel for this project. Prof. Stephen Gould, a world-level expert in optimization and computer vision will provide additional advice.
  * Scope: The ANU team will review Xailient’s technical pipeline of facial recognition. There are many components in this pipeline, such as face detection, alignment, facial recognition, and matching score determination, model/architecture selection, loss function, hyperparameters, training data cleaning, learning strategies, and speed/accuracy optimization.
  * The ANU team will conduct an in-depth review of some of them (as mutually agreed between Xailient and ANU, also depending on how much time each will take), identify gaps, and provide technical advice and guidance to improve their real-world efficacy.
  * The advice given by the ANU team will be based on technologies available in public domains that can benefit and improve Xailient products.
 
  
* NCI Adapter Scheme Q4 2023 (HPC funding scheme)
  * I am a **Chief Investigator (CI)** / **Delegated lead CI** (2023/10/01‐2023/12/31) <!-- The Lead CI is Assoc. Prof. Liang Zheng, and another CI is [Dr. Yunzhong Hou](https://hou-yz.github.io/). -->
  * Project Title: Towards building general-purpose multimodal foundation models
  * Scope: Vision-language pre-training (VLP) has attracted rapidly growing attention in both computer vision and NLP communities due to the emergence of large-scale multimodal foundation models like Contrastive Language-Image Pre-training (CLIP). It is very encouraging to see that many Vision-Language (VL) systems have been deployed in industry. For example, iPhone can generate image captions read by VoiceOver for vision-impaired users. Although multimodal intelligence has been applied in many areas including image-text, core computer vision and video-text tasks, there are still many factors to be considered including robustness to new domains, fairness and responsible AI issues.
  * Aim: One common theme stands out is how to build a general-purpose multi-modal foundation model. We aim to build a foundation model that is stable and generalisable, and can be readily adopted to various downstream tasks, ranging from image-level vision tasks (e.g., image classification, retrieval, and captioning), region-level vision tasks (e.g., object detection and phrase grounding), to pixel-level vision tasks (e.g., segmentation and image generation). In order to build a
general-purpose foundation model, we need a unified model architecture that can be readily scaled up; and when being pre-trained at scale, it can be readily adopted to various downstream computer vision and VL tasks. 

* Grant / Project Award from Oracle for Research
  * **Oracle Cloud credits award** (**AU$48,000**, 2023/07/26‐2024/07/25) <!-- for 366 days -->
  * Research project: Automatic, large-scale screening of failure cases in autonomous driving
  <!-- * Computer perception techniques have been widely used in real-world applications such as autonomous vehicles. While the state-of-the-art methods achieve impressive system accuracy, their real-world robustness is often hindered by corner cases, e.g., a pedestrian partially occluded by an umbrella on a dark day.Instead of letting such cases happen during deployment, in this project, we aim to develop a protocol which can detect potential failure cases through screening millions of different test scenarios via simulation. The identified corner cases are then used to understand system vulnerability and improve accuracy and safety in the end. -->
  * Automotive AI is rapidly replacing human drivers by enabling autonomous vehicles that use sensors to gather data about their surroundings. State-of-the-art methods achieve impressive system accuracy; however, technology is not always perfect, and this imperfection might be magnified  when it comes to safety-critical applications like autonomous driving. Potential failure cases include severe occlusions, extreme weather, low lighting conditions, strange human appearances and poses, which will compromise system performance and even lead to undesirable consequences.
  * In this research project, we aim to build a scalable simulator and diffusion models to generate millions of different test scenarios from which we detect potential failure cases of existing pedestrian and vehicle detection systems. This is to understand the vulnerability of driverless systems, and finally to improve its accuracy and safety through another round of training with these corner cases. The test scenarios cover all possible potential failure cases by varying scene and/or street layout, weather and lighting conditions, human body sizes, visual appearances of human subjects, different levels of occlusions, different levels of sensor noise, etc. More complicated test scenarios are produced by a mixture of several aforementioned individual test scenarios. These different test scenarios are able to reflect the real and sometimes unpredictable driving scenarios on the road. After being identified by running our pedestrian and/or vehicle detectors, failure cases are then summarised and analysed, and the possible reasons why the model is vulnerable to them are identified. Similar failure environments are further explored to find more closely related failure cases. Finally, we fine-tune our pedestrian and vehicle detectors to improve their robustness against difficult test scenarios.

* Research grant: Sharing early insights for more resilient communities
  * I am a **Co-Investigator (Co-I)** (**AU$71,089**, 2023/04/19‐2023/10/31)
  * Research Assistant: [Saswat Panda](https://biology.anu.edu.au/people/professional-staff/saswat-panda) (ANU)
  * This project is a joint initiative with the Southern NSW Drought Resilience Adoption and Innovation Hub and forms part of the Australian Government’s Agricultural Innovation Hubs Program.
  * The project is being conducted by the University of Canberra, Australian National University, Charles Sturt University, and University of Wollongong.
  * The project is to develop and test ‘early warning’ indicators for loss of resilience following challenging climate-related events. These indicators will be used to develop a resource that can be used to by the wide range of organisations and services to identify communities in the early stages of resilience loss and provide targeted support to agricultural communities.
  * Through understanding early warning signs that individuals and communities are at risk of resilience loss, we could inform policy and support service interventions earlier. This would provide communities with the necessary support to mitigate wider and longer lasting resilience loss related to the impacts of a climate events, which in turn, would reduce the overall harm to lives and livelihoods and facilitate resilience building across physical, psychological, social, economic, domains.
 
* Industry research sponsorship (Active Intelligence Corp.): Detecting anomalies in video footage (stage 3)
  * I am a **Principal Investigator** / **Project Lead** (**AU$40,013**, 2023/07/01‐2024/03/01)
  * Project: Human‐related anomaly detection in surveillance videos (stage 3)
  * The objective of Stage 3 is to further enhance the capabilities of human-related anomaly detection in surveillance videos, building upon the progress achieved in Stages 1 and 2.
  * This stage focuses on refining algorithms, improving system adaptability, and integrating advanced features for more robust anomaly detection and classification.

* Industry research sponsorship (Active Intelligence Corp.): Detecting anomalies in video footage (stage 2)
  * I am a **Principal Investigator** / **Project Lead** (**AU$108,628**, 2022/07/01‐2023/06/30)
  * Project: Human‐related anomaly detection in surveillance videos (stage 2)
  * In the second stage of our project on human-related anomaly detection in surveillance videos, we aim to enhance the technology's capabilities. This involves refining normal behavior modeling for adaptability across diverse environments and incorporating feedback from the first stage. We will implement state-of-the-art anomaly detection algorithms, leveraging machine learning for continuous improvement and heightened sensitivity.
  * The project also includes the development of robust event classification mechanisms, real-time alert systems, and features for in-depth post-event analysis and reporting. Special attention will be given to tailoring the technology for specific scenarios, such as shoplifting detection in retail, identification of unauthorized personnel, and improved fall detection in elderly care facilities.
  * Additionally, we will explore the integration of sensor data, including audio and environmental inputs, to augment video-based anomaly detection and provide a more comprehensive understanding of complex situations. These enhancements aim to further advance the technology's applications in retail security, secure areas monitoring, elderly care facilities, and public safety in crowded places. Rigorous testing and validation will ensure the reliability and robustness of the system, reinforcing its role in enhancing security, safety, and situational awareness in modern surveillance systems.
 
* Industry research sponsorship (Active Intelligence Corp.): Detecting anomalies in video footage
  * I am a **Principal Investigator** / **Project Lead** (**AU$135,706**, 2021/07/01‐2022/06/30)
  * Project: Human‐related anomaly detection in surveillance videos
  * Human-related anomaly detection in surveillance videos is a technology used to identify unusual or suspicious behavior involving people within a monitored environment. It is a subfield of video analytics and computer vision that aims to automatically detect deviations from normal human behavior in real-time or from recorded video footage. The primary goal of human-related anomaly detection is to enhance security, safety, and situational awareness in various applications, such as public safety, retail, transportation, and critical infrastructure protection.
  * Key components of human-related anomaly detection in surveillance videos include: normal behavior modeling, anomaly detection algorithms, event classification, real-time alerts, post-event analysis, adaptability, etc.
  * Human-related anomaly detection can be applied in various scenarios, such as identifying potential shoplifting in retail stores, spotting unauthorized personnel in secure areas, detecting accidents or falls in elderly care facilities, or monitoring crowded places for public safety. The technology relies on a combination of machine learning, computer vision, and sensor data to make these determinations, and it plays a vital role in enhancing security and safety in modern surveillance systems.

 

<!-- <h2>Personal statement</h2> -->

<!-- I am a hardworking, passionate and self-disciplined researcher. -->
  
<!-- I have worked with the <font color="blue">large-scale volumetric</font> action recognition datasets which are bigger by several orders of magnitude compared to ImageNet.  -->

<!-- I have learnt to deal efficiently with complex projects given <font color="blue">limited computational resources</font> (albeit HPC-based). -->

<!-- I have a solid background to work on <font color="blue">cross-dataset problems</font>, I understand very well (i) concepts of distribution shift between datasets (ii) issues with generalization to new class concepts (iii) issues underpinning model failures due to out-of-distribution situations (iv) issues resulting from low sample counts, and so forth.  -->

<!-- My knowledge gained from past research experience span several <font color="blue">computer vision and machine learning disciplines</font> including camera projective geometry, graph neural networks, and optimal transportation (apart from action recognition and few-shot learning). -->

<!-- I have delivered a number of strong scientific ideas as witnessed by my (i) academic outputs (papers in high quality venues) and (ii) practical knowledge of action recognition / anomaly detection systems (patents). -->

<!-- <p>&nbsp;</p> -->
<!-- <h2>Research interests</h2> -->

<!-- My interests include action recognition in videos, anomaly detection, video image processing, one- and few-shot learning, deep learning, tensor learning and domain adaptation. -->
